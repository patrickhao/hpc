{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数MLC过程可以看作张量函数之间的转换，下面关注计算图之间的高层优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for deferring annotation parsing in TVMScript\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax, topi\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面看一个简单的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModule:\n",
    "    @R.function\n",
    "    def main(x: Tensor((3, 4), \"float32\"), y: Tensor((3, 4), \"float32\")):\n",
    "        with relax.dataflow():\n",
    "            lv0 = relax.multiply(x, y)\n",
    "            gv0 = relax.add(lv0, y)\n",
    "            relax.output(gv0)\n",
    "        return gv0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包含一个带有两个图层`op`的`relax`函数，其中包含`relax.multiply`和`relax.add`。我们的目标是找到这两个运算符并将它们替换为一个`relax.ewise_fma`运算符的调用\n",
    "\n",
    "首先检查构成`MyModule`的数据结构。 每个`IRModule`都包含一组函数，函数体由一组称为抽象语法树（AST）的数据结构组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relax_func = MyModule[\"main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relax.expr.Function"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(relax_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[relax.expr.Var(0x27bc180), relax.expr.Var(0x27c8ed0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relax_func.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数包含一个返回值表达式，和函数中的一组binding blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relax.expr.SeqExpr"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_body = relax_func.body\n",
    "type(func_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[relax.expr.DataflowBlock(0x278cde0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_body.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_block = func_body.blocks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据流块有两个binding，对于`lv0`和`gv0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[relax.expr.VarBinding(0x2842ed0), relax.expr.VarBinding(0x28436d0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataflow_block.bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "binding = dataflow_block.bindings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个binding都有一个对应于绑定左侧的var(lv0, gv0)，并且每个binding的右侧是他的value，每个value对应一个relax.Call节点，表示对元函数的调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallNode(Op(relax.multiply), [relax.expr.Var(0x27bc180), relax.expr.Var(0x27c8ed0)], (nullptr), [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binding.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改写程序可以通过递归遍历`MyModule`的`AST`，并生成转换后的`AST`来实现。我们当然可以直接使用构建AST的`python API`来做到这一点。但是，我们可以使用额外的工具支持来简化流程。下面的代码块遵循一种称为访问者模式(visitor pattern)的设计模式，它允许我们访问每个`AST`节点并将它们重写为转换后的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "    <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "        lv0: Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(x, y)\n",
       "        gv0: Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>ewise_fma(x, y, y)\n",
       "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv0)\n",
       "    <span style=\"color: #008000; font-weight: bold\">return</span> gv0\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@relax.expr_functor.mutator\n",
    "class EwiseFMARewriter(relax.PyExprMutator):\n",
    "    def visit_call_(self, call):\n",
    "        call = self.visit_expr_post_order(call)\n",
    "        add_op = tvm.ir.Op.get(\"relax.add\")\n",
    "        multiply_op = tvm.ir.Op.get(\"relax.multiply\")\n",
    "        ewise_fma_op = tvm.ir.Op.get(\"relax.ewise_fma\")\n",
    "\n",
    "        if call.op != add_op:\n",
    "            return call\n",
    "\n",
    "        value = self.lookup_binding(call.args[0])\n",
    "        if not isinstance(value, relax.Call) or value.op != multiply_op:\n",
    "            return call\n",
    "\n",
    "        fma_call = relax.Call(\n",
    "            ewise_fma_op, [value.args[0], value.args[1], call.args[1]], None, None\n",
    "        )\n",
    "        return fma_call\n",
    "\n",
    "\n",
    "updated_fn = EwiseFMARewriter().visit_expr(MyModule[\"main\"])\n",
    "updated_fn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果将`gv01`重写为融合运算符，但将`lv0`留在代码中。我们可以使用`remove_all_unused`来进一步简化代码块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "    <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "        gv0: Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>ewise_fma(x, y, y)\n",
       "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv0)\n",
       "    <span style=\"color: #008000; font-weight: bold\">return</span> gv0\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relax.analysis.remove_all_unused(updated_fn).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面尝试在端到端模型上进行尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "mlp_params = pkl.load(open(\"fasionmnist_mlp_params.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>dense(x, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">0</span>])\n",
       "            lv1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">1</span>])\n",
       "            lv2: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv1)\n",
       "            lv3: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>dense(lv2, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">2</span>])\n",
       "            lv4: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv3, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">3</span>])\n",
       "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv4\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model():\n",
    "    bb = relax.BlockBuilder()\n",
    "    x = relax.Var(\"x\", (1, 784), relax.DynTensorType(2, \"float32\"))\n",
    "    w0 = relax.const(mlp_params[\"w0\"], \"float32\")\n",
    "    b0 = relax.const(mlp_params[\"b0\"], \"float32\")\n",
    "    w1 = relax.const(mlp_params[\"w1\"], \"float32\")\n",
    "    b1 = relax.const(mlp_params[\"b1\"], \"float32\")\n",
    "\n",
    "    with bb.function(\"main\", [x]):\n",
    "        with bb.dataflow():\n",
    "            lv0 = bb.emit(relax.op.dense(x, w0))\n",
    "            lv1 = bb.emit(relax.op.add(lv0, b0))\n",
    "            lv2 = bb.emit(relax.op.relu(lv1))\n",
    "            lv3 = bb.emit(relax.op.dense(lv2, w1))\n",
    "            lv4 = bb.emit(relax.op.add(lv3, b1))\n",
    "            gv = bb.emit_output(lv4)\n",
    "        bb.emit_func_output(gv)\n",
    "\n",
    "    return bb.get()\n",
    "\n",
    "MLPModel = create_model()\n",
    "MLPModel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标是“融合”dense和add算子到一起，以下代码通过以下步骤实现：\n",
    "+ 识别dense和add算法\n",
    "+ 生成另一个调用dense和add算子的子函数\n",
    "+ 将dense和add替换为融合后的子函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_dense_add0</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: Tensor((<span style=\"color: #008000\">128</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>dense(x, w)\n",
       "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv, b)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_dense_add1</span>(x1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b1: Tensor((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>dense(x1, w1)\n",
       "            gv1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv1, b1)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv1)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv1\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x2: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv11: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> fused_dense_add0(x2, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">0</span>], meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">1</span>])\n",
       "            lv2: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relax<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv11)\n",
       "            lv4: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> fused_dense_add1(lv2, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">2</span>], meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">3</span>])\n",
       "            gv2: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv4\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv2)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv2\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@relax.expr_functor.mutator\n",
    "class DenseAddFusor(relax.PyExprMutator):\n",
    "    def __init__(self, mod: IRModule) -> None:\n",
    "        super().__init__()\n",
    "        self.mod_ = mod\n",
    "        # cache pre-defined ops\n",
    "        self.add_op = tvm.ir.Op.get(\"relax.add\")\n",
    "        self.dense_op = tvm.ir.Op.get(\"relax.nn.dense\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def transform(self) -> IRModule:\n",
    "        for global_var, func in self.mod_.functions.items():\n",
    "            if not isinstance(func, relax.Function):\n",
    "                continue\n",
    "            # avoid already fused primitive functions\n",
    "            if \"Primitive\" in func.attrs.keys() and func.attrs[\"Primitive\"] != 0:\n",
    "                continue\n",
    "            updated_func = self.visit_expr(func)\n",
    "            updated_func = relax.analysis.remove_all_unused(updated_func)\n",
    "            self.builder_.update_func(global_var, updated_func)\n",
    "\n",
    "        return self.builder_.get()\n",
    "\n",
    "    def visit_call_(self, call):\n",
    "        call = self.visit_expr_post_order(call)\n",
    "\n",
    "        def match_call(node, op):\n",
    "            if not isinstance(node, relax.Call):\n",
    "                return False\n",
    "            return node.op == op\n",
    "\n",
    "        # pattern match dense => add\n",
    "        if not match_call(call, self.add_op):\n",
    "            return call\n",
    "\n",
    "        value = self.lookup_binding(call.args[0])\n",
    "        if value is None:\n",
    "            return call\n",
    "\n",
    "        if not match_call(value, self.dense_op):\n",
    "            return call\n",
    "\n",
    "        x = value.args[0]\n",
    "        w = value.args[1]\n",
    "        b = call.args[1]\n",
    "\n",
    "        # construct a new fused primitive function\n",
    "        param_x = relax.Var(\"x\", x.shape_, x._checked_type_)\n",
    "        param_w = relax.Var(\"w\", w.shape_, w._checked_type_)\n",
    "        param_b = relax.Var(\"b\", b.shape_, b._checked_type_)\n",
    "\n",
    "        bb = relax.BlockBuilder()\n",
    "\n",
    "        fn_name = \"fused_dense_add%d\" % (self.counter)\n",
    "        self.counter += 1\n",
    "        with bb.function(fn_name, [param_x, param_w, param_b]):\n",
    "            with bb.dataflow():\n",
    "                lv0 = bb.emit(relax.op.nn.dense(param_x, param_w))\n",
    "                gv = bb.emit_output(relax.op.add(lv0, param_b))\n",
    "            bb.emit_func_output(gv)\n",
    "\n",
    "        # Add Primitive attribute to the fused funtions\n",
    "        fused_fn = bb.get()[fn_name].with_attr(\"Primitive\", 1)\n",
    "        global_var = self.builder_.add_func(fused_fn, fn_name)\n",
    "\n",
    "        # construct call into the fused function\n",
    "        return relax.Call(global_var, [x, w, b], None, None)\n",
    "\n",
    "@tvm.ir.transform.module_pass(opt_level=2, name=\"DeseAddFuse\")\n",
    "class FuseDenseAddPass:\n",
    "    \"\"\"The wrapper for the LowerTensorIR pass.\"\"\"\n",
    "    def transform_module(self, mod, ctx):\n",
    "        return DenseAddFusor(mod).transform()\n",
    "\n",
    "\n",
    "MLPFused = FuseDenseAddPass()(MLPModel)\n",
    "MLPFused.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们创建了两个前缀为`fuse_dense_add`的子函数。这些子函数包含有融合后算子的计算信息。这种重写的替代方法是简单地为融合运算符创建一个单独的原始操作（如ewise_fma）。但是，当我们尝试融合更多运算符时，可能存在指数级数量的组合。将融合操作分组在一起的子函数为后续的`pass`保留了原始信息，进而便于分析，无需为每个融合`pattern`引入专用的高级运算符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "融合后的`IRModule`仅包含对图层`op`的调用。 为了进一步进行底层优化和代码生成，我们需要将这些高级原语运算转换为相应的`TensorIR`函数（或调用库函数）。\n",
    "\n",
    "以下代码将图层算子重新映射到相应的`TensorIR`函数。 在这里，我们利用`Mutator`中的内部`block builder`并使用`call_te`返回转换后的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">relu</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], compute: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;relu&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                i0_1, i1_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i0_1, i1_1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(compute[i0_1, i1_1])\n",
       "                compute[i0_1, i1_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(rxplaceholder[i0_1, i1_1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> fused_dense_add0(x, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">0</span>], meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">1</span>])\n",
       "            lv2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(relu, (lv1,), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            lv4: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> fused_dense_add1(lv2, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">2</span>], meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">3</span>])\n",
       "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv4\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_dense_add0</span>(x1: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: Tensor((<span style=\"color: #008000\">128</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(dense, (x1, w), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            gv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(add, (lv, b), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv1)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv1\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_dense_add1</span>(x2: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w1: Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b1: Tensor((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(dense1, (x2, w1), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            gv2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(add1, (lv3, b1), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv2)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv2\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">add1</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[<span style=\"color: #008000\">10</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;add1&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                ax0, ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[ax0, ax1], rxplaceholder_1[ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[ax0, ax1])\n",
       "                T_add[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> rxplaceholder[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder_1[ax1]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">dense1</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_matmul_NT: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;dense1&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>]})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                i, j, k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, i2])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i, k], rxplaceholder_1[j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[i, j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder[i, k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[j, k]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">add</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[<span style=\"color: #008000\">128</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;add&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                ax0, ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[ax0, ax1], rxplaceholder_1[ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[ax0, ax1])\n",
       "                T_add[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> rxplaceholder[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder_1[ax1]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">dense</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_matmul_NT: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;dense&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>]})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">784</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                i, j, k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, i2])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i, k], rxplaceholder_1[j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[i, j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder[i, k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[j, k]\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@relax.expr_functor.mutator\n",
    "class LowerToTensorIR(relax.PyExprMutator):\n",
    "    def __init__(self, mod: IRModule, op_map) -> None:\n",
    "        super().__init__()\n",
    "        self.mod_ = mod\n",
    "        self.op_map = {\n",
    "            tvm.ir.Op.get(k): v for k, v in op_map.items()\n",
    "        }\n",
    "\n",
    "\n",
    "    def visit_call_(self, call):\n",
    "        call = self.visit_expr_post_order(call)\n",
    "\n",
    "        if call.op in self.op_map:\n",
    "            return self.op_map[call.op](self.builder_, call)\n",
    "        return call\n",
    "\n",
    "    def transform(self) -> IRModule:\n",
    "        for global_var, func in self.mod_.functions.items():\n",
    "            if not isinstance(func, relax.Function):\n",
    "                continue\n",
    "            updated_func = self.visit_expr(func)\n",
    "            self.builder_.update_func(global_var, updated_func)\n",
    "\n",
    "        return self.builder_.get()\n",
    "\n",
    "\n",
    "def map_dense(bb, call):\n",
    "    x, w = call.args\n",
    "    return bb.call_te(topi.nn.dense, x, w)\n",
    "\n",
    "def map_add(bb, call):\n",
    "    a, b = call.args\n",
    "    return bb.call_te(topi.add, a, b)\n",
    "\n",
    "def map_relu(bb, call):\n",
    "    return bb.call_te(topi.nn.relu, call.args[0])\n",
    "\n",
    "\n",
    "op_map = {\n",
    "  \"relax.nn.dense\": map_dense,\n",
    "  \"relax.add\": map_add,\n",
    "  \"relax.nn.relu\": map_relu\n",
    "}\n",
    "\n",
    "@tvm.ir.transform.module_pass(opt_level=0, name=\"LowerToTensorIR\")\n",
    "class LowerToTensorIRPass:\n",
    "    \"\"\"The wrapper for the LowerTensorIR pass.\"\"\"\n",
    "    def transform_module(self, mod, ctx):\n",
    "        return LowerToTensorIR(mod, op_map).transform()\n",
    "\n",
    "\n",
    "MLPModelTIR = LowerToTensorIRPass()(MLPFused)\n",
    "MLPModelTIR.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，在上面的代码中。`fused_dense_add0`和`fused_dense_add1`仍然是上层`relax`函数，它们调用相应的`TensorIR dense`和`add`函数。 我们可以将它们变成一个单一的`TensorIR`函数，然后可以用于后续优化和代码生成阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #1E90FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_dense_add0</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], w: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], b: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[<span style=\"color: #008000\">128</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;fused_dense_add0&quot;</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        T_matmul_NT <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">784</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                i, j, k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, i2])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[i, k], w[j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[i, j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> x[i, k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> w[j, k]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                ax0, ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(T_matmul_NT[ax0, ax1], b[ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[ax0, ax1])\n",
       "                T_add[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> b[ax1]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">relu</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], compute: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;relu&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                i0_1, i1_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[i0_1, i1_1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(compute[i0_1, i1_1])\n",
       "                compute[i0_1, i1_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(rxplaceholder[i0_1, i1_1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">fused_dense_add1</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], w: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], b: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[<span style=\"color: #008000\">10</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;fused_dense_add1&quot;</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        T_matmul_NT <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                i, j, k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, i2])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[i, k], w[j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[i, j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[i, j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> x[i, k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> w[j, k]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                ax0, ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(T_matmul_NT[ax0, ax1], b[ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[ax0, ax1])\n",
       "                T_add[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[ax0, ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> b[ax1]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #1E90FF\">main</span>(x: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> Tensor(<span style=\"color: #008000; font-weight: bold\">None</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>, ndim <span style=\"color: #AA22FF; font-weight: bold\">=</span> <span style=\"color: #008000\">2</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(fused_dense_add0, (x, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">0</span>], meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">1</span>]), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            lv2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(relu, (lv1,), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            lv4 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(fused_dense_add1, (lv2, meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">2</span>], meta[relay<span style=\"color: #AA22FF; font-weight: bold\">.</span>Constant][<span style=\"color: #008000\">3</span>]), (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            gv: Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv4\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLPModelFinal = relax.transform.FuseTIR()(MLPModelTIR)\n",
    "MLPModelFinal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide outputs\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "img, label = next(iter(test_loader))\n",
    "img = img.reshape(1, 28, 28).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Sandal\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class:\", class_names[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModule Prediction: Sandal\n"
     ]
    }
   ],
   "source": [
    "ex = relax.vm.build(MLPModelFinal, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "data_nd = tvm.nd.array(img.reshape(1, 784))\n",
    "\n",
    "nd_res = vm[\"main\"](data_nd)\n",
    "\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MLPModule Prediction:\", class_names[pred_kind[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba081e9997e81855924de74792aadcd790df3c3d18fa78a445749b56473709c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
